{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scraping two YouTube accounts using python libraries.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ami-nam/LocalRepo/blob/main/Data%20Scraping%20from%20the%20Web/Scraping%20YouTube%20accounts%20with%20python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE07fxu3MLOe"
      },
      "source": [
        "# Scraping two YouTube accounts using python libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCkSzp06MPjz"
      },
      "source": [
        "## Let us master the art of scraping two YouTube accounts and report their subscriber's difference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSTAtR8JMWJ4"
      },
      "source": [
        "![alt text](https://assets.entrepreneur.com/content/3x2/2000/20180117155526-youtube.jpeg?width=700&crop=2:1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djUxccO6Mbjx"
      },
      "source": [
        "**In this article, we will use the python requests library and BeautifulSoup to scrape raw data from unrefined HTML source code.**\n",
        "\n",
        "The most useful libraries required for web scraping are:\n",
        "\n",
        "1. [Beautiful Soup.](https://www.crummy.com/software/BeautifulSoup/bs4/doc/?source=post_page---------------------------)\n",
        "\n",
        "2. [Requests](https://2.python-requests.org/en/master/?source=post_page---------------------------).\n",
        "\n",
        "Let's get started, this tutorial is divided into four parts each part is listed down below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skRTv_iWZUVQ"
      },
      "source": [
        "\n",
        "\n",
        "I have chosen to scrape two YouTube accounts, named \"[PewDiePie](https://www.youtube.com/channel/UC-lHJZR3Gqxm24_Vd_AJ5Yw)\" and \"[T-Series](https://www.youtube.com/user/tseries),\" and report the subscriber difference. I have been a diehard fan of PewDiePie for the past 4 years, and he was the number one YouTuber in terms of his subscribers, up until the last six to ten months. At this point, T-series arrived and eventually outperformed and dethroned PewDiePie, setting a record and becoming the most subscribed channel (apart from YouTube’s own music channel). I did not like T-Series taking over, or how the sub-gap between T-Series and PewDiePie continues to increase drastically. In addition, this subscription gap was one of the biggest controversies in YouTube’s history, which is another reason why I chose to scrape this data. I have a strong feeling that eventually PewDiePie will surpass T-Series again, and remain as the number one YouTuber forever. Scraping these accounts and determining the subscriber gap will allow me to state with accuracy how close PewDiePie is to regain the title of the most subscribed YouTuber.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03SLx2EUKm6K"
      },
      "source": [
        "To find the HTML tags, you must locate both \"[PewDiePie](https://www.youtube.com/channel/UC-lHJZR3Gqxm24_Vd_AJ5Yw)\" and \"[T-Series](https://www.youtube.com/user/tseries),\" YouTube channels. After locating the channels find for the total numbers of subscribers on the page, which would be displayed right below their respected channel names. When you get this right-click on that subscribers and then click on Inspect option or (Ctrl-Shift-I), a new page along with the selected tag will open to your right next to the page. And there you go you can see the HTML tag and the element. Below I have included a screenshot of PewDiePie's channel along with the tags. The same continues to T-Series's channel as it is a mechanical process, follow the same to do it, so which is why I just included only PewDiePie's channel.\n",
        "\n",
        "<center><img src=\"http://i63.tinypic.com/2nkpet1.png\" width = 900 height = 500></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jcNfFy6Qx20"
      },
      "source": [
        "In the above screenshot the tags are \"yt-formatted-string\" it's a custom YouTube tag, unfortunately, this doesn't work. I mean that when I scraped this the soup.findAll method returned an empty pair of brackets ([ ]) and I don't know why maybe it's because of the custom tag and not the regular HTML tag. So, solving this, I just had to look at what is actually being scraped meaning I tried to see what BeautifulSoup is actually returning then I got the answer, soup is actually printing a bit different from the raw HTML, then I searched for the keyword i.e. Subscribers (96, 197, 730) inside the soup using Ctrl-F and then I got the actual HTML tags and the elements. Shown below is the final screenshot of the problem. The actual HTML tag is span and the class is \"yt-subscription-button-subscriber-count-branded-horizontal subscribed yt-uix-tooltip.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx1judwfQcqK"
      },
      "source": [
        "\n",
        "\n",
        "<center><img src=\"http://i63.tinypic.com/s6850m.png\"  width = 800 height = 500></center>\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsLBhRUWqj8I"
      },
      "source": [
        "Let us import few important libraries such as Requests and BeautifulSoup. Let us store the URL of the professor in the variable named \"url\" and \"url2\". The URL of the website can be found here: \"PewDiePie\" and \"T-Series\". Here we use the requests library by passing \"url\" and \"url2\" as a parameter, be careful don't run this multiple times. If you get like Response 200 then its success, if you get something else then there is something wrong with maybe the code or your browser I don't know.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAW65Je1bYoq"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsaM5Djub7bv"
      },
      "source": [
        "url = 'https://www.youtube.com/user/PewDiePie'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qJNC1T7qdck"
      },
      "source": [
        "url2 = 'https://www.youtube.com/user/tseries'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bxta4rTcFhJ"
      },
      "source": [
        "page = requests.get(url)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvihGXapqfhq"
      },
      "source": [
        "page2 = requests.get(url2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5Ad-3jbcMvK",
        "outputId": "b54fa8a7-3c98-4e37-9c22-186918db3808",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(page)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XFASJ85-C6e",
        "outputId": "69ec2e7d-c5bf-409c-e1cc-161439fc9e04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(page2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMpgAq-bamcp"
      },
      "source": [
        "Next, we use the BeautifulSoup library by passing the page.text as a parameter and using the HTML parser. You can try to print the soup, but printing the soup doesn't give you the answer, rather it contains huge chunks of HTML data, so I decided not to show it here. You can then copy the HTML tag and class if any, and then place it inside the soup.findAllmethod."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6MzwvRkcO5A"
      },
      "source": [
        "soup = BeautifulSoup(page.text, \"html.parser\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSHm0Bv2qnYM"
      },
      "source": [
        "soup2 = BeautifulSoup(page2.text, \"html.parser\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIWAyJkWpNIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aece1af-267d-44c5-8a05-84aad8d2768b"
      },
      "source": [
        "pew = soup.findAll(\"span\", {\"class\": \"yt-subscription-button-subscriber-count-branded-horizontal subscribed yt-uix-tooltip\"})"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-382b7f4cea78>:1: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n",
            "  pew = soup.findAll(\"span\", {\"class\": \"yt-subscription-button-subscriber-count-branded-horizontal subscribed yt-uix-tooltip\"})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY4RcghdrOIO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da1e3f8b-c15e-40da-c118-f22d71760676"
      },
      "source": [
        "tseries = soup2.findAll(\"span\", {\"class\": \"yt-subscription-button-subscriber-count-branded-horizontal subscribed yt-uix-tooltip\"})"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-9f2b61aa5304>:1: DeprecationWarning: Call to deprecated method findAll. (Replaced by find_all) -- Deprecated since version 4.0.0.\n",
            "  tseries = soup2.findAll(\"span\", {\"class\": \"yt-subscription-button-subscriber-count-branded-horizontal subscribed yt-uix-tooltip\"})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LDpE0aNpYki",
        "outputId": "6df7d243-87a7-4015-e415-7d3a0dc12e2c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(pew)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHCU_7F2rXTO",
        "outputId": "d7d356f1-0530-422f-d5b5-1cd71b3ca2fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(tseries)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3e556wUapLv"
      },
      "source": [
        "Here we remove all the HTML tags and convert it to a text format, this can be done with the help of get_text method placed inside a for loop. This converts the HTML into the text format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q_M7xh2pd1K"
      },
      "source": [
        "for subs in pew:\n",
        "  print(subs.get_text())"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POQvvW7ZrcJG"
      },
      "source": [
        "for subs1 in tseries:\n",
        "  print(subs1.get_text())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z0Nt32K9YBP"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vNt72EFiujK"
      },
      "source": [
        "Formatting the scraped data was not an easy task because the values that were returned were a series of numbers separated by commas. Puzzled by this formatting issue, I used the website [stack overflow](https://stackoverflow.com/questions/16233593/how-to-strip-comma-in-python-string?source=post_page---------------------------), which helped me through one of their forums. In that forum, they discussed replace (): gets rid of the commas, and the format (): helps in formatting the values. This removal of commas must be done because it was difficult to perform subtraction with commas. Additionally, these values were of type string and they needed to be explicitly converted to an integer by typecasting because it is not possible to subtract two strings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wPFB_hru04Y",
        "outputId": "710f4277-0aec-4d4c-aef2-047af4c5e055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "pewdiepie = subs.get_text().replace(\",\", \"\")\n",
        "pewdiepie"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'subs' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0649130e80a0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpewdiepie\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpewdiepie\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'subs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFFWPirau8dO",
        "outputId": "85abbe1b-e5bc-4271-ac0a-c8b648254d45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "tseries = subs1.get_text().replace(\",\", \"\")\n",
        "tseries"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'subs1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-4b53252dd847>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubs1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtseries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'subs1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHAaluvAs9Ii"
      },
      "source": [
        "difference = int(tseries) - int(pewdiepie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Fw-3DstvQNs",
        "outputId": "0f46c9c4-8750-47be-9afd-24fc194e347f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "difference"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3859472"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCmc8fUqjbCC",
        "outputId": "9c4bf918-fc7b-4bc8-8cac-465585636f59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"The sub gap between T-series and PewDiePie is  ==> \"'{:,}'.format(difference))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sub gap between T-series and PewDiePie is  ==> 3,859,472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2ykuTAZa550"
      },
      "source": [
        "After typecasting, it was a one-step process of subtracting the two variables which contained integers in them. Hence, this is how I successfully obtained the sub difference by scraping the two YouTube channels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sQwCsvna7jJ"
      },
      "source": [
        "Thank you guys for spending your time reading my tutorial, stay tuned for more updates. Let me know what is your opinion about this tutorial in the comment section below. Also if you have any doubts regarding the code, comment section is all yours. Have a nice day."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbippKuA9auY"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}